{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorítmos de Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais com Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes Neurais nada mais são que técnicas computacionais desenvolvidas em torno de modelos matemáticos que remontam \n",
    "a estrutura neural presente em organismos inteligentes. É importante considerar que esses modelos adquirem conhecimento por experiência.\n",
    "\n",
    "Para entender essas técnicas, é preciso se compreender o aspecto biológico. Sabe-se que o sistema nervoso é formado por um conjunto complexo de células, conhecidos como neurônios, estes têm um papel importante para o funcionamento, comportamento e raciocínio do ser humano. Eles são formados por dentritos (terminais de entrada), corpo central e axônios (Terminais de Saída).\n",
    "\n",
    "Artificialmente falando, uma rede desta é composta por várias unidades de processamento, com funcionamento simplificado, conectadas por canais de comunicação com determinado peso associado. Cada unidade opera apenas sobre os dados recebidos pelas conexões, essas operações consistem na relação dos sinais apresentados na entrada e, cada um destes é multiplicado por um peso (Indicando a influência na saída), uma soma ponderada produz um nível de atividade que, se exceder um limite (threshold), a unidade produz uma resposta.\n",
    "\n",
    "No geral os pesos das conexões são ajustados aos padrões, de modo que elas aprendam, ou sejam treinadas, por exemplos. A organização básica dá-se em camadas, com unidades conectadas à de camada superior. É importante frisar que o aprendizado ocorre quando a rede neural atinge uma solução generalizada para uma classe de problemas.\n",
    "\n",
    "![img](http://conteudo.icmc.usp.br/pessoas/andre/research/neural/image/camadas_an.gif)\n",
    "\n",
    "Dentre os tipos de aprendizado, o que se relaciona diretamente com o problema deste trabalho é o aprendizado não supervisionado, que pressupõe uma auto-organização. \n",
    "\n",
    "Há dois diferentes modos de executar a correção de pesos em um ciclo : Modo padrão (correção a cada apresentação de um exemplo do conjunto de treinamento) e modo batch, quando apenas uma correção é feita por ciclo, de modo que todos os exemplos do conjunto de treinamento são mostrados à rede ( calcula erro médio e faz correção dos pesos).\n",
    "\n",
    "De maneira geral,\n",
    "\n",
    "Dado um conjunto Dado um conjunto com N exemplos encontrar a função que minimize uma função de erro estabelecida, ou seja, o treinamento de uma rede neural consiste em decidir qual o mínimo de uma função de erro.\n",
    "\n",
    "### neural_network.MLPRegressor\n",
    "\n",
    "[documentação](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "\n",
    "O sckitlearn apresenta o MLPRegressor, que treina sem nenhuma função de ativação na camada de saída. Ele usa o erro quadrado como função de perda e tem como saída um conjunto de valores contínuos. Além disso, ele suporta regressão em que uma amostra tenha mais que um destino. Quanto algorítmo, se usado tem disponível o parâmetro alpha para evitar overfitting, penabilizando pesos de magnitude maior. \n",
    "\n",
    "Esse algoritmo é treinado de maneira iterativa, desde que em cada detalhe do tempo as derivadadas parciais da função do MLPRegressor são computadas e atualizam os parâmetros. Este modelo otimiza a perda quadrática usando LBFGS ou descendente de gradiente estocástico. Como o esperado, suporta diversos parâmetros, como:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hidden_layer_sizes : tuple, length = n_layers - 2, padrão (100,) - Possível controlar o número de camadas em layers\n",
    "* activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’ - Função de ativação para a camada oculta\n",
    "\n",
    "      'identity', útil para implementar o gargalo linear, retorna f (x) = x\n",
    "      'logistic', a função sigmoide logística, retorna f (x) = 1 / (1 + exp (-x)).\n",
    "      'tanh', a função bronzeada hiperbólica, retorna f (x) = tanh (x).\n",
    "      'relu', a função de unidade linear retificada, retorna f (x) = max (0, x)\n",
    "\n",
    "* solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’ -  Solucionador para otimização de peso.\n",
    "\n",
    "      'lbfgs' é um otimizador da família dos métodos  quasi-Newton.\n",
    "      'sgd' refere-se à descida de gradiente estocástica.\n",
    "      'adam' refere-se a um otimizador baseado em gradiente estocástico proposto por Kingma, Diederik e Jimmy Ba\n",
    "      \n",
    "      Nota: O padrão 'adam' funciona muito bem em conjuntos de dados relativamente grandes (com milhares de amostras de treinamento ou mais) em termos de tempo de treinamento e pontuação de validação. Para conjuntos de dados pequenos, no entanto, 'lbfgs' podem convergir mais rapidamente e ter melhor desempenho.\n",
    "      \n",
    "* alpha : float, optional, default 0.0001 - Parâmetro de penalidade L2 (termo de regularização).\n",
    "* batch_size : int, optional, default ‘auto’ - Tamanho de minibatches para otimizadores estocásticos (sgd ou adam).\n",
    "\n",
    "      Quando definido como \"auto\", batch_size = min (200, n_samples)\n",
    "      \n",
    "* learning_rate : {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’ - Taxa de aprendizado para atualizações de peso.\n",
    "\n",
    "      'constant' é taxa de aprendizado constante dada por 'learning_rate_init'.\n",
    "      'invscaling' diminui gradualmente a taxa de aprendizado learning_rate_ em cada passo de tempo 't' usando um expoente de escala inversa de 'power_t'. effective_learning_rate = learning_rate_init / pow (t, poder_t)\n",
    "      'adaptive' mantém taxa de aprendizado constante para 'learning_rate_init', desde que a perda de treinamento continue diminuindo.       \n",
    "      \n",
    "* learning_rate_init : double, optional, default 0.001 - Taxa inicial de aprendizado usada. Controla o tamanho da etapa na atualização dos pesos. Usado somente quando solver = 'sgd' ou 'adam'.\n",
    "\n",
    "* power_t : double, optional, default 0.5 - Expoente para taxa de aprendizado de escala inversa. \n",
    "\n",
    "      Usado para atualizar a taxa efetiva de aprendizado quando o valor do aprendizado é definido como 'invscaling'. Usado somente quando solver = 'sgd'.\n",
    "\n",
    "* max_iter : int, optional, default 200 - Número máximo de iterações.\n",
    "\n",
    "      Solver itera até a convergência (determinada por 'tol') ou esse número de iterações. Para resolvedores estocásticos ('sgd', 'adam'), observe que isso determina o número de épocas (quantas vezes cada ponto de dados será usado), não o número de etapas de gradiente.\n",
    "      \n",
    "* shuffle : bool, optional, default True - Embaralhar amostras em cada iteração. Usado somente quando solver = 'sgd' ou 'adam'.\n",
    "\n",
    "* random_state : int, RandomState instance or None, optional, default None\n",
    "\n",
    "      Se inteito, random_state é a semente usada pelo gerador de números aleatórios; Se a instância RandomState, random_state é o gerador de números aleatórios; Se Nenhum, o gerador de números aleatórios é a instância RandomState usada por np.random .\n",
    "\n",
    "* tol : float, optional, default 1e-4 - Tolerância para a otimização\n",
    "\n",
    "      Quando a perda ou pontuação não está melhorando pelo menos durante duas iterações consecutivas, a menos que o learning_rate seja ajustado para 'adaptativo', a convergência é considerada atingida e o treinamento pára.\n",
    " \n",
    "* verbose : bool, optional, default False - Deve imprimir mensagens de progresso no stdout.\n",
    "* warm_start : bool, optional, default False \n",
    "\n",
    "      Quando definido como True, reutiliza a solução da chamada anterior para ajustar como inicialização, caso contrário, basta apagar a solução anterior.\n",
    "      \n",
    "* momentum : float, default 0.9 - Definir momento para atualização de descida de gradiente. Deve estar entre 0 e 1. Apenas usado quando solver = 'sgd'.\n",
    "\n",
    "* nesterovs_momentum : boolean, default True - Para usar o momentum de Nesterov. Usado somente quando solver = 'sgd' e momentum> 0.\n",
    "\n",
    "* early_stopping : bool, default False - Parada antecipada para finalizar o treinamento quando a pontuação de validação não estiver melhorando. \n",
    " \n",
    "      Se definido como verdadeiro, anula automaticamente 10% dos dados de treinamento como validação e encerra o treinamento quando a pontuação de validação não estiver melhorando pelo menos durante duas épocas consecutivas. Apenas efetivo quando solver = 'sgd' ou 'adam'\n",
    "      \n",
    "* validation_fraction : float, optional, default 0.1 - Proporção de dados de treinamento a serem definidos como um conjunto de validação para interrupção antecipada. \n",
    "\n",
    "      Deve estar entre 0 e 1. Apenas usado se early_stopping for True\n",
    "      \n",
    "* eta_1 : float, optional, default 0.9 - Taxa de decaimento exponencial para estimativas do primeiro momento vetorial em adam deve estar em [0, 1]. \n",
    "\n",
    "      Usado somente quando solver = 'adam'\n",
    "beta_2 : float, opcional, padrão 0,999\n",
    "A taxa de decaimento exponencial para estimativas do segundo momento vetorial em adam deve estar em [0, 1). Usado somente quando solver = 'adam'\n",
    "epsilon : float, opcional, padrão 1e-8\n",
    "Valor para estabilidade numérica em adam. Usado somente quando solver = 'adam'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
